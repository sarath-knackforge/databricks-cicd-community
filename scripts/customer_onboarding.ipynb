{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6944f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# =========================================\n",
    "# CONFIG\n",
    "# =========================================\n",
    "FLASK_API_URL = \"https://databricks-approval-backend.onrender.com/customers\"   # ‚úÖ CHANGE THIS\n",
    "CATALOG = \"mlops_prod\"\n",
    "CONTROL_SCHEMA = \"platform\"\n",
    "CUSTOMER_TABLE = f\"{CATALOG}.{CONTROL_SCHEMA}.customers_master\"\n",
    "\n",
    "TIMEOUT = 45  # API timeout in seconds\n",
    "\n",
    "print(f\"‚úÖ Target customer table: {CUSTOMER_TABLE}\")\n",
    "print(f\"‚úÖ Reading customers from API: {FLASK_API_URL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4eb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================\n",
    "# 1Ô∏è‚É£ CALL FLASK API\n",
    "# =========================================\n",
    "try:\n",
    "    response = requests.get(FLASK_API_URL, timeout=TIMEOUT)\n",
    "    response.raise_for_status()\n",
    "    api_customers = response.json()\n",
    "    print(f\"‚úÖ Received {len(api_customers)} customers from API\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"‚ùå Failed to fetch customers from API: {e}\")\n",
    "\n",
    "# =========================================\n",
    "# 2Ô∏è‚É£ LOAD EXISTING CUSTOMERS FROM DATABRICKS\n",
    "# =========================================\n",
    "existing_df = spark.table(CUSTOMER_TABLE).select(\"customer_id\")\n",
    "existing_ids = {row[\"customer_id\"] for row in existing_df.collect()}\n",
    "print(f\"‚úÖ Found {len(existing_ids)} existing customers in Databricks\")\n",
    "\n",
    "# =========================================\n",
    "# 3Ô∏è‚É£ FILTER ONLY NEW CUSTOMERS\n",
    "# =========================================\n",
    "new_customers = [\n",
    "    c for c in api_customers\n",
    "    if c[\"customer_id\"] not in existing_ids\n",
    "]\n",
    "\n",
    "if not new_customers:\n",
    "    print(\"‚úÖ No new customers found ‚Äî onboarding skipped.\")\n",
    "    dbutils.notebook.exit(\"NO_NEW_CUSTOMERS\")\n",
    "\n",
    "print(f\"üöÄ New customers to onboard: {len(new_customers)}\")\n",
    "\n",
    "# =========================================\n",
    "# 4Ô∏è‚É£ CONVERT NEW CUSTOMERS TO SPARK DF\n",
    "# =========================================\n",
    "spark_schema = T.StructType([\n",
    "    T.StructField(\"customer_id\", T.StringType(), False),\n",
    "    T.StructField(\"customer_name\", T.StringType(), True),\n",
    "    T.StructField(\"schema_name\", T.StringType(), True),\n",
    "    T.StructField(\"cloud_provider\", T.StringType(), True),\n",
    "    T.StructField(\"data_path\", T.StringType(), True),\n",
    "    T.StructField(\"is_active\", T.BooleanType(), True),\n",
    "    T.StructField(\"onboarding_status\", T.StringType(), True),\n",
    "    T.StructField(\"last_pipeline_run\", T.TimestampType(), True),\n",
    "    T.StructField(\"created_by\", T.StringType(), True),\n",
    "    T.StructField(\"created_at\", T.TimestampType(), True),\n",
    "    T.StructField(\"updated_at\", T.TimestampType(), True),\n",
    "])\n",
    "\n",
    "spark_rows = []\n",
    "for c in new_customers:\n",
    "    spark_rows.append((\n",
    "        c[\"customer_id\"],\n",
    "        c.get(\"customer_name\"),\n",
    "        c.get(\"schema_name\"),\n",
    "        c.get(\"cloud_provider\"),\n",
    "        c.get(\"data_path\"),\n",
    "        bool(c.get(\"is_active\", 1)),\n",
    "        \"ACTIVE\",\n",
    "        None,\n",
    "        \"flask_api_onboarding\",\n",
    "        None,\n",
    "        None\n",
    "    ))\n",
    "\n",
    "spark_new_df = spark.createDataFrame(spark_rows, schema=spark_schema) \\\n",
    "    .withColumn(\"created_at\", F.current_timestamp()) \\\n",
    "    .withColumn(\"updated_at\", F.current_timestamp())\n",
    "\n",
    "# =========================================\n",
    "# 5Ô∏è‚É£ INSERT INTO customers_master\n",
    "# =========================================\n",
    "spark_new_df.write.mode(\"append\").saveAsTable(CUSTOMER_TABLE)\n",
    "\n",
    "print(f\"‚úÖ Inserted {spark_new_df.count()} new customers into {CUSTOMER_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================\n",
    "# 2Ô∏è‚É£ LOAD EXISTING CUSTOMERS FROM DATABRICKS\n",
    "# =========================================\n",
    "existing_df = spark.table(CUSTOMER_TABLE).select(\"customer_id\")\n",
    "existing_ids = {row[\"customer_id\"] for row in existing_df.collect()}\n",
    "print(f\"‚úÖ Found {len(existing_ids)} existing customers in Databricks\")\n",
    "\n",
    "# =========================================\n",
    "# 3Ô∏è‚É£ FILTER ONLY NEW CUSTOMERS\n",
    "# =========================================\n",
    "new_customers = [\n",
    "    c for c in api_customers\n",
    "    if c[\"customer_id\"] not in existing_ids\n",
    "]\n",
    "\n",
    "if not new_customers:\n",
    "    print(\"‚úÖ No new customers found ‚Äî onboarding skipped.\")\n",
    "    dbutils.notebook.exit(\"NO_NEW_CUSTOMERS\")\n",
    "\n",
    "print(f\"üöÄ New customers to onboard: {len(new_customers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26018ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================\n",
    "# 4Ô∏è‚É£ CONVERT NEW CUSTOMERS TO SPARK DF\n",
    "# =========================================\n",
    "spark_schema = T.StructType([\n",
    "    T.StructField(\"customer_id\", T.StringType(), False),\n",
    "    T.StructField(\"customer_name\", T.StringType(), True),\n",
    "    T.StructField(\"schema_name\", T.StringType(), True),\n",
    "    T.StructField(\"cloud_provider\", T.StringType(), True),\n",
    "    T.StructField(\"data_path\", T.StringType(), True),\n",
    "    T.StructField(\"is_active\", T.BooleanType(), True),\n",
    "    T.StructField(\"onboarding_status\", T.StringType(), True),\n",
    "    T.StructField(\"last_pipeline_run\", T.TimestampType(), True),\n",
    "    T.StructField(\"created_by\", T.StringType(), True),\n",
    "    T.StructField(\"created_at\", T.TimestampType(), True),\n",
    "    T.StructField(\"updated_at\", T.TimestampType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark_rows = []\n",
    "for c in new_customers:\n",
    "    spark_rows.append((\n",
    "        c[\"customer_id\"],\n",
    "        c.get(\"customer_name\"),\n",
    "        c.get(\"schema_name\"),\n",
    "        c.get(\"cloud_provider\"),\n",
    "        c.get(\"data_path\"),\n",
    "        bool(c.get(\"is_active\", 1)),\n",
    "        \"ACTIVE\",\n",
    "        None,\n",
    "        \"flask_api_onboarding\",\n",
    "        None,\n",
    "        None\n",
    "    ))\n",
    "\n",
    "spark_new_df = spark.createDataFrame(spark_rows, schema=spark_schema) \\\n",
    "    .withColumn(\"created_at\", F.current_timestamp()) \\\n",
    "    .withColumn(\"updated_at\", F.current_timestamp())\n",
    "\n",
    "# =========================================\n",
    "# 5Ô∏è‚É£ INSERT INTO customers_master\n",
    "# =========================================\n",
    "spark_new_df.write.mode(\"append\").saveAsTable(CUSTOMER_TABLE)\n",
    "\n",
    "print(\"‚úÖ ‚úÖ CUSTOMER ONBOARDING FROM FLASK API COMPLETED ‚úÖ\")\n",
    "print(f\"‚úÖ Inserted {spark_new_df.count()} new customers into {CUSTOMER_TABLE}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
